{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy<2.0.0 in /root/miniconda3/lib/python3.10/site-packages (1.26.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<2.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy version is 1.*.*, you're good to go!\n"
          ]
        }
      ],
      "source": [
        "# make sure we're using numpy 1.*.*\n",
        "import numpy as np\n",
        "\n",
        "if (np.__version__).startswith(\"1.\"):\n",
        "    print(\"Numpy version is 1.*.*, you're good to go!\")\n",
        "else:\n",
        "    raise ValueError(\"Please restart your runtime using the above instructions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Optional\n",
        "WANDB_API_KEY = \"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "# Optional\n",
        "OPENPIPE_API_KEY = \"\"\n",
        "if OPENPIPE_API_KEY:\n",
        "    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv pip install openpipe-art openpipe --prerelease allow --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import random\n",
        "from typing import TypedDict\n",
        "from typing import Literal\n",
        "import string\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "WINNING_VALUE = 128\n",
        "\n",
        "# Class that keeps track of state for a single game of 2048\n",
        "class TwentyFortyEightGame(TypedDict):\n",
        "    id: str\n",
        "    board: list[list[int | None]]\n",
        "\n",
        "# Randomly populates a cell on the board with a 2 or 4\n",
        "def populate_random_cell(game: TwentyFortyEightGame) -> None:\n",
        "    all_clear_coordinates = [\n",
        "        (i, j)\n",
        "        for i in range(len(game[\"board\"]))\n",
        "        for j in range(len(game[\"board\"][i]))\n",
        "        if game[\"board\"][i][j] is None\n",
        "    ]\n",
        "    random_clear_coordinates = random.choice(all_clear_coordinates)\n",
        "    # 90% chance to populate a 2, 10% chance to populate a 4\n",
        "    game[\"board\"][random_clear_coordinates[0]][random_clear_coordinates[1]] = (\n",
        "        2 if random.random() < 0.9 else 4\n",
        "    )\n",
        "\n",
        "# Generates a new game of 2048\n",
        "def generate_game(board_length: int = 4) -> TwentyFortyEightGame:\n",
        "    # random 6 character string\n",
        "    id = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
        "    game = {\n",
        "        \"id\": id,\n",
        "        \"board\": [[None for _ in range(board_length)] for _ in range(board_length)],\n",
        "    }\n",
        "\n",
        "    # populate two random cells\n",
        "    populate_random_cell(game)\n",
        "    populate_random_cell(game)\n",
        "\n",
        "    return game\n",
        "\n",
        "# Renders the board in a human-readable format\n",
        "def render_board(game: TwentyFortyEightGame) -> str:\n",
        "    board = game[\"board\"]\n",
        "    # print something like this:\n",
        "    # _    | 2    | _    | 4\n",
        "    # 4    | 8    | 2    | 16\n",
        "    # 16   | 32   | 64   | 128\n",
        "    # _    | 2    | 2    | 4\n",
        "    # where _ is an empty cell\n",
        "\n",
        "    max_cell_width = max(\n",
        "        [len(str(cell)) for row in board for cell in row if cell is not None]\n",
        "    )\n",
        "\n",
        "    board_str = \"\"\n",
        "    for row in board:\n",
        "        # pad the cells with spaces to make them the same width\n",
        "        board_str += \"|\".join(\n",
        "            [\n",
        "                str(cell).rjust(max_cell_width)\n",
        "                if cell is not None\n",
        "                else \"_\".rjust(max_cell_width)\n",
        "                for cell in row\n",
        "            ]\n",
        "        )\n",
        "        board_str += \"\\n\"\n",
        "    return board_str\n",
        "\n",
        "\n",
        "# condense, privileging matches at the start of the sequence\n",
        "# sequences should be passed starting with cells that are the furthest in the direction in which the board is being condensed\n",
        "def condense_sequence(sequence: list[int | None]) -> list[int | None]:\n",
        "    condensed_sequence = []\n",
        "\n",
        "    gapless_sequence = [cell for cell in sequence if cell is not None]\n",
        "\n",
        "    i = 0\n",
        "    while i < len(gapless_sequence):\n",
        "        if (\n",
        "            i + 1 < len(gapless_sequence)\n",
        "            and gapless_sequence[i] == gapless_sequence[i + 1]\n",
        "        ):\n",
        "            condensed_sequence.append(gapless_sequence[i] * 2)\n",
        "            i += 2\n",
        "        else:\n",
        "            condensed_sequence.append(gapless_sequence[i])\n",
        "            i += 1\n",
        "\n",
        "    # pad the sequence with None at the end\n",
        "    return condensed_sequence + [None] * (4 - len(condensed_sequence))\n",
        "\n",
        "# Condenses the board in a given direction\n",
        "def condense_board(\n",
        "    game: TwentyFortyEightGame, direction: Literal[\"left\", \"right\", \"up\", \"down\"]\n",
        ") -> None:\n",
        "    if direction == \"left\":\n",
        "        for row in game[\"board\"]:\n",
        "            condensed_row = condense_sequence(row)\n",
        "            for i in range(len(row)):\n",
        "                row[i] = condensed_row[i]\n",
        "\n",
        "    if direction == \"right\":\n",
        "        for row in game[\"board\"]:\n",
        "            reversed_row = row[::-1]\n",
        "            # reverse the row before and after condensing\n",
        "            condensed_row = condense_sequence(reversed_row)[::-1]\n",
        "            for i in range(len(row)):\n",
        "                row[i] = condensed_row[i]\n",
        "\n",
        "    if direction == \"up\":\n",
        "        for col_index in range(len(game[\"board\"][0])):\n",
        "            column = [row[col_index] for row in game[\"board\"]]\n",
        "\n",
        "            condensed_column = condense_sequence(column)\n",
        "            for row_index in range(len(column)):\n",
        "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
        "\n",
        "    if direction == \"down\":\n",
        "        for col_index in range(len(game[\"board\"][0])):\n",
        "            column = [row[col_index] for row in game[\"board\"]]\n",
        "            reversed_column = column[::-1]\n",
        "            condensed_column = condense_sequence(reversed_column)[::-1]\n",
        "            for row_index in range(len(column)):\n",
        "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
        "\n",
        "\n",
        "# Applies an agent move to the game board\n",
        "def apply_agent_move(game: TwentyFortyEightGame, move_xml: str) -> None:\n",
        "    direction = None\n",
        "    # parse the move\n",
        "    try:\n",
        "        root = ET.fromstring(move_xml)\n",
        "        direction = root.text\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Invalid xml\")\n",
        "\n",
        "    if direction not in [\"left\", \"right\", \"up\", \"down\"]:\n",
        "        raise ValueError(\"Invalid direction\")\n",
        "\n",
        "    condense_board(game, direction)\n",
        "\n",
        "    populate_random_cell(game)\n",
        "\n",
        "# Returns the maximum cell value on the board\n",
        "def max_cell_value(game: TwentyFortyEightGame) -> int:\n",
        "    return max([cell for row in game[\"board\"] for cell in row if cell is not None])\n",
        "\n",
        "# Returns True if the game is finished\n",
        "def check_game_finished(game: TwentyFortyEightGame) -> bool:\n",
        "    if max_cell_value(game) >= WINNING_VALUE:\n",
        "        return True\n",
        "\n",
        "    # check if any cell is empty\n",
        "    if any(cell is None for row in game[\"board\"] for cell in row):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Returns the sum of all the cell values on the board\n",
        "def total_board_value(game: TwentyFortyEightGame) -> int:\n",
        "    return sum([cell for row in game[\"board\"] for cell in row if cell is not None])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/sky_workdir/src/art/local/state.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth  # type: ignore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-17 02:26:12 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
            "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
            "Unsloth: vLLM's KV Cache can use up to 59.84 GB. Also swap space = 6 GB.\n",
            "INFO 04-17 02:26:23 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 04-17 02:26:24 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n",
            "INFO 04-17 02:26:24 cuda.py:229] Using Flash Attention backend.\n",
            "WARNING 04-17 02:26:24 registry.py:335] `mm_limits` has already been set for model=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
            "INFO 04-17 02:26:25 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 04-17 02:26:25 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 04-17 02:26:25 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d61e4c600624226bb4dfe5cc3f16773",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8db7fbee4cd4cf295e420678dbc8543",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-17 02:26:27 model_runner.py:1115] Loading model weights took 2.2265 GB\n",
            "INFO 04-17 02:26:27 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-17 02:26:29 worker.py:267] Memory profiling takes 2.51 seconds\n",
            "INFO 04-17 02:26:29 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.78) = 62.06GiB\n",
            "INFO 04-17 02:26:29 worker.py:267] model weights take 2.23GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 2.71GiB; the rest of the memory reserved for KV Cache is 56.98GiB.\n",
            "INFO 04-17 02:26:30 executor_base.py:111] # cuda blocks: 103733, # CPU blocks: 10922\n",
            "INFO 04-17 02:26:30 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 50.65x\n",
            "INFO 04-17 02:26:37 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:44<00:00,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-17 02:27:22 model_runner.py:1562] Graph capturing finished in 45 secs, took 1.30 GiB\n",
            "INFO 04-17 02:27:22 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 55.35 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Unsloth 2025.3.19 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "import art\n",
        "from dotenv import load_dotenv\n",
        "from openpipe.client import AsyncOpenPipe\n",
        "import random\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Initialize the server\n",
        "api = art.LocalAPI(\n",
        "    # Normally we don't want to run the server in-process, but for the output\n",
        "    # to show up properly on Google Colab we'll enable this.\n",
        "    in_process=True\n",
        ")\n",
        "\n",
        "class CustomConfig(BaseModel):\n",
        "    litellm_model_name: str | None = None\n",
        "\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=\"011\",\n",
        "    project=\"2048-dev\",\n",
        "    base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    # To run on a T4, we need to override some config defaults.\n",
        "    _internal_config=art.dev.InternalModelConfig(\n",
        "        init_args=art.dev.InitArgs(\n",
        "            max_seq_length=8192,\n",
        "        ),\n",
        "        engine_args=art.dev.EngineArgs(\n",
        "            enforce_eager=True,\n",
        "            gpu_memory_utilization=0.8,\n",
        "            num_scheduler_steps=1,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "await model.register(api)\n",
        "\n",
        "gpt_4o_mini = art.Model(\n",
        "    name=\"gpt-4o-mini\",\n",
        "    project=\"2048-dev\",\n",
        "    config=CustomConfig(\n",
        "        litellm_model_name=\"openai/gpt-4o-mini\",\n",
        "    ),\n",
        ")\n",
        "await gpt_4o_mini.register(api)\n",
        "\n",
        "\n",
        "# Optional logging client\n",
        "op_client = AsyncOpenPipe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "from art.utils.get_trajectory_messages import get_trajectory_messages\n",
        "\n",
        "import openai\n",
        "import time\n",
        "import math\n",
        "import requests\n",
        "from litellm import acompletion\n",
        "\n",
        "\n",
        "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
        "async def rollout(\n",
        "    model: art.Model, step: int = 0, is_validation: bool = False\n",
        ") -> art.Trajectory:\n",
        "\n",
        "    game = generate_game()\n",
        "\n",
        "    move_number = 0\n",
        "\n",
        "    trajectory = art.Trajectory(\n",
        "        messages_and_choices=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\",\n",
        "            }\n",
        "        ],\n",
        "        reward=0,\n",
        "    )\n",
        "\n",
        "\n",
        "    while True:\n",
        "\n",
        "        trajectory.messages_and_choices.append(\n",
        "            {\"role\": \"user\", \"content\": render_board(game)}\n",
        "        )\n",
        "\n",
        "        requested_at = int(time.time() * 1000)\n",
        "        messages = get_trajectory_messages(trajectory)\n",
        "\n",
        "        try:\n",
        "            model_id = model.config.litellm_model_name if isinstance(model.config, CustomConfig) else f\"openai/{model.name}\"\n",
        "            chat_completion = await acompletion(\n",
        "                base_url=model.base_url,\n",
        "                api_key=model.api_key,\n",
        "                model=model_id,\n",
        "                messages=messages,\n",
        "                max_completion_tokens=128,\n",
        "            )\n",
        "            last_completion = chat_completion\n",
        "        except openai.LengthFinishReasonError as e:\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print(\"caught exception generating chat completion\", e)\n",
        "            raise e\n",
        "\n",
        "        try:\n",
        "            if op_client.api_key:\n",
        "                await op_client.report(\n",
        "                    requested_at=requested_at,\n",
        "                    received_at=int(time.time() * 1000),\n",
        "                    req_payload={\n",
        "                        \"model\": model.name,\n",
        "                        \"messages\": messages,\n",
        "                        \"metadata\": {\n",
        "                            \"game_id\": game[\"id\"],\n",
        "                            \"notebook-id\": \"2048\",\n",
        "                            \"step\": str(step),\n",
        "                            \"validation\": str(is_validation),\n",
        "                            \"move_number\": str(move_number),\n",
        "                        },\n",
        "                    },\n",
        "                    resp_payload=chat_completion,\n",
        "                    status_code=200,\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error reporting to OpenPipe: {e}\")\n",
        "\n",
        "\n",
        "        choice = chat_completion.choices[0]\n",
        "        content = choice.message.content\n",
        "        assert isinstance(content, str)\n",
        "        trajectory.messages_and_choices.append(choice)\n",
        "        \n",
        "\n",
        "        try:\n",
        "            apply_agent_move(game, content)\n",
        "            move_number += 1\n",
        "        except ValueError:\n",
        "            trajectory.reward = -1\n",
        "            break\n",
        "\n",
        "        if check_game_finished(game):\n",
        "            max_value = max_cell_value(game)\n",
        "            board_value = total_board_value(game)\n",
        "            trajectory.metrics[\"max_value\"] = max_value\n",
        "            trajectory.metrics[\"board_value\"] = board_value\n",
        "\n",
        "            if max_value < WINNING_VALUE:\n",
        "                # scale max value logarithmically between 0 for 2 and 1 for WINNING_VALUE\n",
        "                max_value_reward = (math.log(max_value, 2) - 1) / (\n",
        "                    math.log(WINNING_VALUE, 2) - 1\n",
        "                )\n",
        "                # scale board value logarithmically between 0 for 2 * 16 and 1 for WINNING_VALUE * 16\n",
        "                board_value_reward = (math.log(board_value, 2) - 1) / (\n",
        "                    math.log(WINNING_VALUE * 16, 2) - 1\n",
        "                )\n",
        "                # combine the two rewards, with max value having a higher weight\n",
        "                trajectory.reward = max_value_reward + (board_value_reward * 0.2)\n",
        "            else:\n",
        "                # double reward if the agent wins\n",
        "                trajectory.reward = 2\n",
        "                # add random number between 0 and .001\n",
        "                trajectory.reward += random.random() * 0.001\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        if op_client.api_key:\n",
        "            await op_client.update_log_metadata(\n",
        "                filters=[\n",
        "                    {\n",
        "                        \"field\": \"completionId\",\n",
        "                        \"equals\": last_completion.id,\n",
        "                    }\n",
        "                ],\n",
        "                metadata={\n",
        "                    \"reward\": str(trajectory.reward),\n",
        "                    \"reward_assigned\": \"true\",\n",
        "                },\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating log metadata: {e}\")\n",
        "\n",
        "    return trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f8db8b5bde54adaa1b965c2b09b9704",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mopenpipe\u001b[0m (\u001b[33mopenpipe-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/root/sky_workdir/examples/2048/wandb/run-20250417_022821-010</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/openpipe-team/2048-dev/runs/010' target=\"_blank\">010</a></strong> to <a href='https://wandb.ai/openpipe-team/2048-dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/openpipe-team/2048-dev' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/openpipe-team/2048-dev/runs/010' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev/runs/010</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Packed 2 trajectories into 1 sequences of length 6144\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7771a2c2704f50a00058f0f7e25a6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,000,000 | Num Epochs = 3 | Total steps = 30,000,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 14,966,784/3,000,000,000 (0.50% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "903249324d074fcba95050afb36324bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Deleted checkpoint ./.art/2048-dev/models/010/0000\n",
            "Packed 2 trajectories into 2 sequences of length 4096\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22003c73666b4df385127e05ed8bb93e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c64548e974084df18a272d6088249db9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Deleted checkpoint ./.art/2048-dev/models/010/0001\n",
            "Packed 2 trajectories into 2 sequences of length 6144\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3ed2e69a30a4a96bd14be09244a77df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb6bcbf0ddb048949b3b29f3360338b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Deleted checkpoint ./.art/2048-dev/models/010/0002\n",
            "Packed 2 trajectories into 2 sequences of length 6144\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d55eb21de0b4fbabf0c133a4dc54697",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28feb37f641d4357bb5b9ed72749dad4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Deleted checkpoint ./.art/2048-dev/models/010/0003\n",
            "Packed 2 trajectories into 2 sequences of length 6144\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bac32d86d1a44e595f694307c80ef99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c375a7576ad42e197bc8f1cbca6b517",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Deleted checkpoint ./.art/2048-dev/models/010/0004\n",
            "Packed 2 trajectories into 2 sequences of length 6144\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3f3d675a9ac4d1dbd02334239eb5aa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "openai_client = model.openai_client()\n",
        "for i in range(await model.get_step(), 6):\n",
        "    train_groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, i, is_validation=False) for _ in range(2)\n",
        "            )\n",
        "            for _ in range(1)\n",
        "        ),\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=1,\n",
        "    )\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(\n",
        "        train_groups,\n",
        "        config=art.TrainConfig(learning_rate=3e-5),\n",
        "        # Lowering the logprob_calculation_chunk_size is a memory saving measure\n",
        "        # to allow longer sequences (up to 4096 tokens) to be processed on a T4.\n",
        "        _config={\"logprob_calculation_chunk_size\": 8},\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17c6944b819c42c48c10d27cfb0d2c7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TrajectoryGroup(trajectories=[Trajectory(messages_and_choices=[{'content': \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\", 'role': 'system'}, {'role': 'user', 'content': '_|_|_|2\\n_|_|_|_\\n_|_|_|2\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|_|_\\n_|_|_|_\\n_|_|_|2\\n_|_|_|4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|2\\n_|_|_|4\\n_|_|_|_\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '4|_|_|_\\n4|_|_|_\\n_|_|_|2\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|_|_|2\\n_|_|_|_\\n_|4|_|_\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|2|_\\n_|_|_|_\\n_|_|_|_\\n8|4|_|2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|4|2|2\\n_|_|_|_\\n_|_|_|_\\n_|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|4|_\\n_|_|_|_\\n_|4|_|_\\n8|2|2|2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|4|4|2\\n_|2|2|_\\n_|_|_|_\\n2|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|8|2|4\\n4|_|_|_\\n_|_|_|_\\n2|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|8|2|4\\n4|_|_|_\\n2|_|4|_\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|_|_\\n8|2|_|_\\n4|_|2|_\\n2|8|4|4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|_|_\\n8|2|_|_\\n4|2|_|2\\n2|8|8|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' _| _| _| _\\n 8| 2| _| _\\n 4| 4| 2| _\\n 2|16| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 8| 2| 2| _\\n 4| 4| _| _\\n 2|16| _| _\\n _| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 8| 2| 2| _\\n 4| 4| _| _\\n 2|16| _| _\\n 4| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 8| 2| 2| _\\n 4| 4| _| _\\n 2|16| _| _\\n 4| 2| 2| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 8| 4| _| _\\n 8| _| 4| _\\n 2|16| _| _\\n 4| 4| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 4| 4| _\\n 2|16| _| _\\n 4| 4| _| 2\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 4| 4| 2\\n 2|16| 2| _\\n 4| 4| _| _\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 2| _\\n 2|16| 2| _\\n 8| _| _| _\\n _| _| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 2\\n 2|16| _| _\\n 8| 2| _| _\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 2\\n 2|16| _| _\\n 8| 2| _| 2\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' _| _| _| 2\\n16| 8| _| _\\n 2|16| _| _\\n 8| 2| 4| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 2\\n 2|16| 2| 4\\n 8| 2| _| _\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 2\\n 2|16| 2| 4\\n 8| 2| _| _\\n _| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' _| _| _| _\\n16| 8| _| 2\\n 2|16| 4| 2\\n 8| 4| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 4\\n 2|16| 2| 4\\n 8| 4| _| 4\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 8\\n 2|16| 2| 4\\n 8| 4| _| _\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 8\\n 2|16| 2| 4\\n 8| 4| 2| _\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| _| _| 2\\n 2| 8| _| _\\n 8|16| 4| 8\\n 2| 4| 4| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 8| 2\\n 2|16| 2| 8\\n 8| 4| _| 4\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| _| 2| _\\n 2| 8| _| 2\\n 8|16| 8| 8\\n 2| 4| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 2| 2\\n 2|16| 8| 8\\n 8| 4| 2| 4\\n 2| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| _\\n 2|16|16| _\\n 8| 4| 2| 4\\n 4| _| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| 4\\n 2|16|16| 2\\n 8| 4| 2| _\\n 4| _| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 8| _\\n 2|32| 2| _\\n 8| 4| 2| _\\n 4| 2| 2| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 8| 2\\n 2|32| 4| _\\n 8| 4| 2| _\\n 4| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 8| 2\\n 2|32| 4| 2\\n 8| 4| 2| _\\n 4| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| _| _\\n 2|32| 8| _\\n 8| 4| 4| 2\\n 4| 2| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 8| 2\\n 2|32| 4| 4\\n 8| 4| 2| _\\n 4| 2| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '16| 8| 4| _\\n 2|32| 8| 2\\n 8| 4| 4| 4\\n 4| 2| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], reward=0.7806754610294885, metrics={'max_value': 32, 'board_value': 104}, metadata={}, logs=[]), Trajectory(messages_and_choices=[{'content': \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\", 'role': 'system'}, {'role': 'user', 'content': '_|_|_|_\\n_|_|_|_\\n_|_|_|_\\n4|_|_|2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|2|_\\n_|_|_|_\\n_|_|_|_\\n4|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|2|_\\n_|_|_|_\\n_|_|_|_\\n4|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '_|_|2|_\\n_|_|_|_\\n2|_|_|_\\n4|2|2|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n_|_|_|_\\n2|_|_|_\\n4|2|4|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n2|_|_|_\\n2|_|_|_\\n4|2|4|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '4|2|4|_\\n2|_|_|_\\n4|2|_|_\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '4|4|4|_\\n2|_|_|_\\n4|_|_|2\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|4|_|_\\n2|_|_|_\\n4|2|2|_\\n_|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '8|4|_|_\\n2|_|_|_\\n4|4|_|_\\n_|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n8|_|_|_\\n2|8|_|_\\n4|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n8|_|2|_\\n2|8|_|_\\n4|2|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|2|_\\n8|_|_|_\\n2|8|_|_\\n4|2|2|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n8|2|_|_\\n2|8|_|_\\n4|2|4|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|_|_|_\\n8|2|2|_\\n2|8|_|_\\n4|2|4|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': '2|2|2|_\\n8|8|4|2\\n2|2|_|_\\n4|_|_|_\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 2| _| _\\n16| 4| 2| _\\n 4| _| _| _\\n 4| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 2| 2| _\\n16| 4| _| _\\n 8| 2| _| _\\n _| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 4| _| _\\n16| 4| _| _\\n 8| 2| _| 2\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n16| _| _| _\\n 8| 8| _| _\\n 2| 2| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n16| _| _| _\\n 8| 8| _| 2\\n 2| 2| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| 2\\n16| _| _| _\\n 8| 8| _| _\\n 2| 2| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| 2| _\\n16| _| _| _\\n 8| 8| _| 2\\n 2| 2| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 4| 2\\n16| 2| _| 4\\n 8| _| _| 2\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| 2\\n16| _| _| 2\\n 8| 8| _| 4\\n 2| 2| 4| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 2| _| _\\n16| 2| _| _\\n16| 4| _| _\\n 4| 4| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' _| _| _| _\\n 4| _| _| _\\n32| 4| _| 2\\n 4| 8| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 4| 2| 4\\n32| 8| _| _\\n 4| _| _| 2\\n _| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 4| 2| 4\\n32| 8| _| 2\\n 4| _| _| _\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| _| 2| _\\n 4| 4| _| 4\\n 2| 8| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 4| 4| 4\\n32| 8| _| 2\\n 4| _| _| _\\n 2| _| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 2| _| _\\n 4| 4| _| 4\\n 2| 8| 4| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 2| 2| _\\n 8| 4| _| _\\n 2| 8| 8| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 2| _| 2\\n 8| 4| 2| _\\n 2| 8| 8| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| 2\\n32| 2| _| _\\n 8| 4| 2| _\\n 2| 8| 8| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 2| 2| _\\n32| 2| _| _\\n 8| 4| 2| _\\n 2|16| 2| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 4| 4| _\\n32| 4| 2| _\\n 8|16| _| 2\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 4| 2\\n32|16| 2| _\\n 8| 2| _| _\\n 2| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 8| 2| _\\n 8|16| 4| _\\n 2| 2| 2| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>left</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 8| 2| _\\n 8|16| 4| 2\\n 4| 4| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 2| 2\\n32|16| 4| 2\\n 8| 4| _| _\\n 4| _| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 8| _| 2\\n 8|16| 2| _\\n 4| 4| 4| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 2| 2\\n32|16| 4| 4\\n 8| 4| _| _\\n 4| _| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| 2\\n32| 8| _| 2\\n 8|16| 2| 4\\n 4| 4| 4| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| 2\\n32| 8| _| 4\\n 8|16| 2| 4\\n 4| 4| 4| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| _| _| _\\n32| 8| 2| 2\\n 8|16| 2| 8\\n 4| 4| 4| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 4| 2\\n32|16| 4| 8\\n 8| 4| _| 2\\n 4| 2| _| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| _| _\\n32|16| _| 2\\n 8| 4| 2| 8\\n 4| 2| 8| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 2| 2\\n32|16| 8| 8\\n 8| 4| _| 2\\n 4| 2| _| 2\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 2| _\\n32|16| _| 2\\n 8| 4| 2| 8\\n 4| 2| 8| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 4| 2\\n32|16| 8| 8\\n 8| 4| _| 4\\n 4| 2| 2| _\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>down</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])), {'role': 'user', 'content': ' 4| 8| 2| _\\n32|16| 4| 2\\n 8| 4| 8| 8\\n 4| 2| 2| 4\\n'}, Choices(finish_reason='stop', index=0, message=Message(content='<move>up</move>', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], reward=0.7822938609371598, metrics={'max_value': 32, 'board_value': 110}, metadata={}, logs=[])], metadata={}, exceptions=[])]\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/board_value</td><td>â–‡â–â–†â–ˆâ–ˆâ–‚</td></tr><tr><td>train/exception_rate</td><td>â–â–â–â–â–â–</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–„â–â–‚â–â–†</td></tr><tr><td>train/loss</td><td>â–â–ƒâ–‚â–â–‚â–ˆ</td></tr><tr><td>train/max_value</td><td>â–ˆâ–‚â–ˆâ–‚â–ˆâ–</td></tr><tr><td>train/policy_loss</td><td>â–â–ƒâ–‚â–â–‚â–ˆ</td></tr><tr><td>train/reward</td><td>â–ƒâ–â–ˆâ–„â–ƒâ–„</td></tr><tr><td>train/reward_std_dev</td><td>â–ˆâ–†â–â–â–ˆâ–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/board_value</td><td>156</td></tr><tr><td>train/exception_rate</td><td>0</td></tr><tr><td>train/grad_norm</td><td>1.24335</td></tr><tr><td>train/loss</td><td>0.30515</td></tr><tr><td>train/max_value</td><td>48</td></tr><tr><td>train/policy_loss</td><td>0.30516</td></tr><tr><td>train/reward</td><td>0.87492</td></tr><tr><td>train/reward_std_dev</td><td>0.09011</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">010</strong> at: <a href='https://wandb.ai/openpipe-team/2048-dev/runs/010' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev/runs/010</a><br> View project at: <a href='https://wandb.ai/openpipe-team/2048-dev' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250417_022821-010/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/root/sky_workdir/examples/2048/wandb/run-20250417_023425-gpt-4o-mini</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/openpipe-team/2048-dev/runs/gpt-4o-mini' target=\"_blank\">gpt-4o-mini</a></strong> to <a href='https://wandb.ai/openpipe-team/2048-dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/openpipe-team/2048-dev' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/openpipe-team/2048-dev/runs/gpt-4o-mini' target=\"_blank\">https://wandb.ai/openpipe-team/2048-dev/runs/gpt-4o-mini</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trajectories = await art.gather_trajectory_groups(\n",
        "    (\n",
        "        art.TrajectoryGroup(rollout(gpt_4o_mini, 0, is_validation=True) for _ in range(2))\n",
        "        for _ in range(1)\n",
        "    ),\n",
        "    pbar_desc=\"gather\",\n",
        "    max_exceptions=1,\n",
        ")\n",
        "print(trajectories)\n",
        "\n",
        "await gpt_4o_mini.log(\n",
        "    trajectories,\n",
        "    split=\"val\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'exceptions'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmarking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_comparison_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_comparison_table\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmarking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BenchmarkedModelKey\n\u001b[0;32m----> 4\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_comparison_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2048-dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBenchmarkedModelKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m009\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBenchmarkedModelKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboard_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(table\u001b[38;5;241m.\u001b[39mto_markdown())\n",
            "File \u001b[0;32m~/sky_workdir/src/art/utils/benchmarking/generate_comparison_table.py:12\u001b[0m, in \u001b[0;36mgenerate_comparison_table\u001b[0;34m(project, benchmark_keys, metrics, api_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_comparison_table\u001b[39m(\n\u001b[1;32m      7\u001b[0m     project: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     benchmark_keys: \u001b[38;5;28mlist\u001b[39m[BenchmarkedModelKey],\n\u001b[1;32m      9\u001b[0m     metrics: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m     api_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./.art\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 12\u001b[0m     benchmarked_models \u001b[38;5;241m=\u001b[39m \u001b[43mload_benchmarked_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m benchmarked_model \u001b[38;5;129;01min\u001b[39;00m benchmarked_models:\n",
            "File \u001b[0;32m~/sky_workdir/src/art/utils/benchmarking/load_benchmarked_models.py:48\u001b[0m, in \u001b[0;36mload_benchmarked_models\u001b[0;34m(project, benchmark_keys, metrics, api_path)\u001b[0m\n\u001b[1;32m     45\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(split_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 48\u001b[0m     trajectory_groups \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_trajectory_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# add \"reward\" to trajectory metrics to ensure it is treated like a metric\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trajectory_group \u001b[38;5;129;01min\u001b[39;00m trajectory_groups:\n",
            "File \u001b[0;32m~/sky_workdir/src/art/utils/trajectory_logging.py:52\u001b[0m, in \u001b[0;36mdeserialize_trajectory_groups\u001b[0;34m(serialized)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeserialize_trajectory_groups\u001b[39m(serialized: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[TrajectoryGroup]:\n\u001b[1;32m     51\u001b[0m     loaded_groups \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(serialized, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mSafeLoader)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [dict_to_trajectory_group(group) \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m loaded_groups]\n",
            "File \u001b[0;32m~/sky_workdir/src/art/utils/trajectory_logging.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeserialize_trajectory_groups\u001b[39m(serialized: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[TrajectoryGroup]:\n\u001b[1;32m     51\u001b[0m     loaded_groups \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(serialized, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mSafeLoader)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdict_to_trajectory_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m loaded_groups]\n",
            "File \u001b[0;32m~/sky_workdir/src/art/utils/trajectory_logging.py:58\u001b[0m, in \u001b[0;36mdict_to_trajectory_group\u001b[0;34m(dict)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdict_to_trajectory_group\u001b[39m(\u001b[38;5;28mdict\u001b[39m: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TrajectoryGroup:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrajectoryGroup(\n\u001b[1;32m     56\u001b[0m         trajectories\u001b[38;5;241m=\u001b[39m[dict_to_trajectory(trajectory) \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectories\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     57\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 58\u001b[0m         exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexceptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     59\u001b[0m     )\n",
            "\u001b[0;31mKeyError\u001b[0m: 'exceptions'"
          ]
        }
      ],
      "source": [
        "from art.utils.benchmarking.generate_comparison_table import generate_comparison_table\n",
        "from art.utils.benchmarking.types import BenchmarkedModelKey\n",
        "\n",
        "table = generate_comparison_table(\n",
        "    project=\"2048-dev\",\n",
        "    benchmark_keys=[\n",
        "        BenchmarkedModelKey(\"009\", \"train\", [0, -2, -1]),\n",
        "        BenchmarkedModelKey(\"gpt-4o-mini\", \"val\")\n",
        "    ],\n",
        "    metrics=[\"reward\", \"max_value\", \"board_value\"]\n",
        ")\n",
        "\n",
        "print(table.to_markdown())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
