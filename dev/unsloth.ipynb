{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-26 19:11:35 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import peft\n",
    "import transformers\n",
    "from typing import cast\n",
    "import vllm\n",
    "\n",
    "class CausallLM(transformers.PreTrainedModel, transformers.GenerationMixin):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading NousResearch/Hermes-2-Theta-Llama-3-8B with actual GPU utilization = 61.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 33.56 GB. Also swap space = 6 GB.\n",
      "INFO 03-26 19:11:43 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-26 19:11:43 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":320}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efde3c6828124f7389138c629fb85a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b5f18bf467429bad9ec3ff3def9a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbdc477b2074505825723746db1bcf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6811a1bc815440459b57d796e197fd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 19:11:44 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 03-26 19:11:44 registry.py:335] `mm_limits` has already been set for model=NousResearch/Hermes-2-Theta-Llama-3-8B, and will be overwritten by the new values.\n",
      "INFO 03-26 19:11:45 model_runner.py:1110] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 03-26 19:11:45 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W326 19:11:45.502029661 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 19:11:45 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e192a6a34c8641e3b90bd49ae4391a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd96a82ac8b4587b39dd7c8d11ba84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0aea3975e43d897184b6167e8fbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e934a8a7274fef812641c6c331e902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 19:12:08 weight_utils.py:270] Time spent downloading weights for NousResearch/Hermes-2-Theta-Llama-3-8B: 23.158234 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186d3e512b4140ce9b3f0c33dec4c016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 19:12:11 model_runner.py:1115] Loading model weights took 5.3246 GB\n",
      "INFO 03-26 19:12:11 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-26 19:12:13 worker.py:267] Memory profiling takes 1.60 seconds\n",
      "INFO 03-26 19:12:13 worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.62) = 48.68GiB\n",
      "INFO 03-26 19:12:13 worker.py:267] model weights take 5.32GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 1.56GiB; the rest of the memory reserved for KV Cache is 41.64GiB.\n",
      "INFO 03-26 19:12:13 executor_base.py:111] # cuda blocks: 21317, # CPU blocks: 3072\n",
      "INFO 03-26 19:12:13 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 41.63x\n",
      "INFO 03-26 19:12:16 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:30<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 19:12:47 model_runner.py:1562] Graph capturing finished in 30 secs, took 5.54 GiB\n",
      "INFO 03-26 19:12:47 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 35.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 4096, padding_idx=128001)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_rank = 32\n",
    "model_name = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "model, tokenizer = cast(\n",
    "    tuple[CausallLM, transformers.PreTrainedTokenizerBase],\n",
    "    unsloth.FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=8192,\n",
    "        load_in_4bit=True,  # False for LoRA 16bit\n",
    "        fast_inference=True,  # Enable vLLM fast inference\n",
    "        # vLLM args\n",
    "        disable_log_requests=True,\n",
    "        disable_log_stats=False,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=0.62,  # Reduce if out of memory\n",
    "        max_lora_rank=lora_rank,\n",
    "        # max_num_seqs=1024,\n",
    "        # enforce_eager=True,\n",
    "        num_scheduler_steps=16,\n",
    "        use_async=True,\n",
    "    ),\n",
    ")\n",
    "vllm_engine = cast(vllm.AsyncLLMEngine, model.vllm_engine)\n",
    "peft_model = cast(\n",
    "    peft.peft_model.PeftModelForCausalLM,\n",
    "    unsloth.FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],  # Remove QKVO if out of memory\n",
    "        lora_alpha=lora_rank,\n",
    "        # Enable long context finetuning\n",
    "        use_gradient_checkpointing=\"unsloth\",  # type: ignore\n",
    "        random_state=3407,\n",
    "    ),\n",
    ")\n",
    "lora_model = cast(peft.tuners.lora.LoraModel, peft_model.base_model)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.worker.worker_base import WorkerWrapperBase\n",
    "from vllm.worker.multi_step_model_runner import MultiStepModelRunner\n",
    "\n",
    "worker = cast(\n",
    "    WorkerWrapperBase, getattr(vllm_engine.engine.model_executor, \"driver_worker\")\n",
    ")\n",
    "multi_step_model_runner: MultiStepModelRunner = worker.model_runner\n",
    "base_model_runner = multi_step_model_runner._base_model_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "config = GRPOConfig(per_device_train_batch_size=8)\n",
    "\n",
    "\n",
    "def reward_func(*_, **__) -> float:\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def generator():\n",
    "    while True:\n",
    "        yield {\"prompt\": \"\"}\n",
    "\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=peft_model,\n",
    "    reward_funcs=[],\n",
    "    args=config,\n",
    "    train_dataset=Dataset.from_list([{\"prompt\": \"\"} for _ in range(1_000_000)]),\n",
    "    # train_dataset=IterableDataset.from_generator(generator),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._save_optimizer_and_scheduler(trainer.args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.create_optimizer_and_scheduler(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 0.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 1.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 2.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 3.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 4.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 5.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 6.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 7.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 8.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 9.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 11.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 12.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 13.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 14.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 15.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 16.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 17.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 18.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 19.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 20.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 21.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 22.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 23.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 24.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 25.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 26.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 27.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 28.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 29.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 30.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 31.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 32.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 33.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 34.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 35.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 36.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 37.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 38.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 39.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 40.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 41.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 42.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 43.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 44.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 45.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 46.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 47.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 48.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 49.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 50.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 51.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 52.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 53.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 54.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 55.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 56.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 57.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 58.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 59.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 60.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 61.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 62.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 63.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 64.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 65.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 66.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 67.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 68.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 69.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 70.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 71.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 72.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 73.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 74.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 75.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 76.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 77.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 78.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 79.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 80.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 81.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 82.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 83.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 84.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 85.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 86.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 87.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 88.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 89.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 90.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 91.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 92.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 93.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 94.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 95.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 96.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 97.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 98.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 99.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 100.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 101.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 102.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 103.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 104.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 105.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 106.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 107.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 108.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 109.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 110.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 111.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 112.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 113.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 114.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 115.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 116.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 117.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 118.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 119.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 120.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 121.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 122.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 123.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 124.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 125.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 126.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 127.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 128.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 129.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 130.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 131.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 132.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 133.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 134.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 135.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 136.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 137.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 138.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 139.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 140.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 141.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 142.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 143.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 144.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 145.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 146.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 147.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 148.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 149.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 150.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 151.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 152.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 153.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 154.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 155.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 156.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 157.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 158.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 159.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 160.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 161.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 162.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 163.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 164.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 165.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 166.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 167.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 168.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 169.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 170.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 171.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 172.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 173.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 174.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 175.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 176.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 177.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 178.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 179.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 180.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 181.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 182.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 183.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 184.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 185.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 186.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 187.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 188.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 189.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 190.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 191.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 192.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 193.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 194.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 195.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 196.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 197.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 198.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 199.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 200.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 201.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 202.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 203.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 204.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 205.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 206.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 207.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 208.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 209.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 210.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 211.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 212.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 213.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 214.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 215.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 216.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 217.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 218.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 219.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 220.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 221.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 222.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 223.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 224.\n",
      "INFO 03-26 18:26:09 async_llm_engine.py:211] Added request 225.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def warmup(request_id: str) -> None:\n",
    "    async for _ in vllm_engine.generate(\n",
    "        prompt={\n",
    "            \"prompt_token_ids\": [0]\n",
    "            * (vllm_engine.engine.scheduler_config.max_model_len - 2)\n",
    "        },\n",
    "        sampling_params=vllm.SamplingParams(max_tokens=2),\n",
    "        request_id=request_id,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "\n",
    "await asyncio.gather(\n",
    "    *(warmup(f\"{i}\") for i in range(vllm_engine.engine.scheduler_config.max_num_seqs))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"55887MiB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.tuners.lora.model.LoraModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peft_model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import models\n",
    "models.llama.LlamaForCausalLM\n",
    "\n",
    "from typing import Protocol\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
